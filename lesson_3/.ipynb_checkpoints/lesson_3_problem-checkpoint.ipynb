{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab Lesson 3: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving **in the desired direction**\n",
    "- $P(0.1)$ of moving in a direction 90Â° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, and Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward of starting state: -0.04\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (1, 0) with action left: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action left: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (1, 0) with action left:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action left:\", env.T[current_state, 1, goal_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.9\n",
      "0.1\n",
      "0.9\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "state = env.sample(current_state, 0)\n",
    "print(state)\n",
    "\n",
    "print(env.T[current_state, 0, state])\n",
    "print(env.T[current_state, 1, state])\n",
    "print(env.T[current_state, 2, state])\n",
    "print(env.T[current_state, 3, state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Value Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the tests on different environment versions but with the same structure: *HugeLavaFloor*, *NiceLavaFloor*, and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "The *value_iteration* function has to be implemented. Notice that the value iteration approach returns a matrix with the value for each state, the function *values_to_policy* automatically converts this matrix into the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "\n",
    "    # KEY IDEA : SOLVE ITERATIVELY VIA DYNAMIC PROGRAMMING -> iterate the Bellman update until convergence\n",
    "    # N.B. studying the convergence to a value that tends to +inf we are guaranteed to find the optimal value for our problem\n",
    "    # also we don't move to another state based on the weight of that node, but on a probability decided by us\n",
    "    # Q(s, a) -> expected total reward for taking action a in the state s\n",
    "    # V(s) -> maximum expected total reward starting from the state s (value function)\n",
    "    # Q(s, a) = R(s, a) + gamma + sum_{s'}(P(s' | s, a) * V(s'))\n",
    "\n",
    "    U_1 = [0] * environment.observation_space.n # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # finche' il counter non raggiunge il massimo di iterazioni passate come parametro\n",
    "    while counter < maxiters:\n",
    "        # aggiorno il counter\n",
    "        counter += 1\n",
    "        # copio gli stati di utility per gli stati S su U\n",
    "        U = U_1.copy()\n",
    "        # massimi cambi nella utility di ogni stati in una iterazione\n",
    "        delta = 0\n",
    "        \n",
    "        # per tutti gli stati presenti nel raggio d'azione\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # creo un parametro per salvare la somma dei valori\n",
    "            sum_value = [0 for _ in range(environment.action_space.n)]\n",
    "            # doppio ciclo che rappresenta la sommatoria\n",
    "            for action in range(environment.action_space.n):         \n",
    "                for state_1 in range(environment.observation_space.n):\n",
    "                    sum_value[action] += env.T[state, action, state_1] * U[state_1]\n",
    "                    \n",
    "                if environment.grid[state] == 'P' or environment.grid[state] == 'G':\n",
    "                    U_1[state] = environment.RS[state]\n",
    "                # equivale a Q(s, a) = R(s, a) + gamma + sum_{s'}(P(s' | s, a) * V(s'))\n",
    "                else:\n",
    "                     U_1[state] = environment.RS[state] + discount * max(sum_value)\n",
    "            \n",
    "            # controllo e riassegnamento di delta\n",
    "            if abs(U_1[state] - U[state]) > delta:\n",
    "                delta = abs(U_1[state] - U[state])\n",
    "    \n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3511/2720712339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nENV RENDER:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_name' is not defined"
     ]
    }
   ],
   "source": [
    "#env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Policy Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the tests on different environment versions but with the same structure: *HugeLavaFloor*, *NiceLavaFloor*, and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=150, discount=0.9, maxviter=10):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "\n",
    "    # simile alla value iteration tranne per il fatto che il calcolo della optimal policy viene calcolato ad ogni step e non in modo piu' generale come la value iteration fa\n",
    "    # e quindi e' piu' dispersiva e meno ottimizzata, anche se piu' precisa#\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)] #initial policy\n",
    "    U = [0 for _ in range(environment.observation_space.n)] #utility array\n",
    "\n",
    "    unchanged = False\n",
    "    for _ in range(maxiters):\n",
    "        for j in range(maxviter):\n",
    "            for state in range(environment.observation_space.n):\n",
    "                result = 0\n",
    "\n",
    "                for state1 in range(environment.observation_space.n):\n",
    "                    result += environment.T[state, policy[state], state1] * U[state1]\n",
    "                if environment.grid[state] == 'P' or environment.grid[state] == 'G':       \n",
    "                    U[state] = environment.RS[state]\n",
    "                else:\n",
    "                    U[state] = (discount * result) + environment.RS[state]\n",
    "        \n",
    "        unchanged = True\n",
    "        \n",
    "        for state in range(environment.observation_space.n):\n",
    "            action_values = [0 for _ in range(environment.action_space.n)]\n",
    "            policy_value = 0\n",
    "            for state1 in range(environment.observation_space.n):\n",
    "                policy_value += environment.T[state, policy[state], state1] * U[state1]\n",
    "                for action in range(environment.action_space.n):\n",
    "                    action_values[action] += environment.T[state, action, state1] * U[state1]\n",
    "            if max(action_values) > policy_value:\n",
    "                policy[state] = np.argmax(action_values)\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            break\n",
    "            \n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.0049\n",
      "\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: VeryBadLavaFloor-v0 \tPolicy Iteration  ########\u001b[0m\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92m===> Your solution is correct!\n",
      "\n",
      "Policy:\n",
      "[['R' 'R' 'R' 'D']\n",
      " ['D' 'L' 'R' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "The following code compares Value Iteration and Policy Iteration by plotting the accumulated rewards of each episode with iterations in the range $[1, 50]$ (might take a long time if not optimized via NumPy). You can perform all the tests on different environment versions but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(environment, policy, max_iteration)** runs an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "\n",
    "maxiters = 49\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "        \n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava floor results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava floor results comparison** \n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
